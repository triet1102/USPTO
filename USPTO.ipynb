{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c16a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04b7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc192cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name=\"bert-base-uncased\"\n",
    "    val_size=0.25\n",
    "    max_len=192\n",
    "    batch_size=32\n",
    "    epochs=6\n",
    "    lr=2e-5\n",
    "    max_lr=1e-3\n",
    "    steps_per_epoch=None\n",
    "    pct_start=0.3\n",
    "    div_factor=1e+2\n",
    "    final_div_factor=1e+4\n",
    "    accumulate=1\n",
    "    patience=3\n",
    "    monitor=\"val_loss\"\n",
    "    seed=42\n",
    "    debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8635ee8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              'model_name': 'bert-base-uncased',\n",
       "              'val_size': 0.25,\n",
       "              'max_len': 192,\n",
       "              'batch_size': 32,\n",
       "              'epochs': 6,\n",
       "              'lr': 2e-05,\n",
       "              'max_lr': 0.001,\n",
       "              'steps_per_epoch': None,\n",
       "              'pct_start': 0.3,\n",
       "              'div_factor': 100.0,\n",
       "              'final_div_factor': 10000.0,\n",
       "              'accumulate': 1,\n",
       "              'patience': 3,\n",
       "              'monitor': 'val_loss',\n",
       "              'seed': 42,\n",
       "              'debug': True,\n",
       "              '__dict__': <attribute '__dict__' of 'CFG' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'CFG' objects>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e774a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseSimilarityDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_params = {\n",
    "            \"max_length\": CFG.max_len,\n",
    "            \"padding\": \"max_length\",\n",
    "            \"truncation\": True\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        anchor = self.df.anchor.iloc[index].lower()\n",
    "        target = self.df.target.iloc[index].lower()        \n",
    "        \n",
    "        tokens = self.tokenizer(anchor + '[SEP]' + target, **self.tokenizer_params)\n",
    "        score = torch.tensor(self.df.score.iloc[index], dtype=torch.float32)\n",
    "        \n",
    "        return (\n",
    "            np.array(tokens[\"input_ids\"]),\n",
    "            np.array(tokens[\"attention_mask\"]),\n",
    "            score\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2040e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseSimilarityTestset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_params = {\n",
    "            \"max_length\": CFG.max_len,\n",
    "            \"padding\": \"max_length\",\n",
    "            \"truncation\": True\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        anchor = self.df.anchor.iloc[index].lower()\n",
    "        target = self.df.target.iloc[index].lower()        \n",
    "        \n",
    "        tokens = self.tokenizer(anchor + '[SEP]' + target, **self.tokenizer_params)\n",
    "        \n",
    "        return (\n",
    "            np.array(tokens[\"input_ids\"]),\n",
    "            np.array(tokens[\"attention_mask\"]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c8f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseSimilarityModelImpl(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.head = nn.Linear(768, 1, bias=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, text, mask):\n",
    "        # calculate bert output\n",
    "        feats = self.bert(text, mask)\n",
    "        # calculate sum of all tokens then divide by the number of tokens\n",
    "        feats = torch.sum(feats[0], 1) / feats[0].shape[1]\n",
    "        feats = self.dropout(feats)\n",
    "        output = self.head(feats)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074633d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseSimilarityModel(pl.LightningModule):\n",
    "    def __init__(self, model, criterion, metric):\n",
    "        super(PhraseSimilarityModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.metric = metric\n",
    "    \n",
    "    def forward(self, text, mask):\n",
    "        return self.model(text, mask)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=CFG.lr)\n",
    "        return self.optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids, mask = batch[0], batch[1]\n",
    "        preds = self.model(ids, mask)\n",
    "        loss = self.criterion(preds.squeeze(1), batch[2])\n",
    "        rmse = self.metric(preds.squeeze(1), batch[2])\n",
    "        logs = {\"train_loss\": loss, \"train_error\": rmse, \"lr\": self.optimizer.param_groups[0]['lr']}\n",
    "        \n",
    "        self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids, mask = batch[0], batch[1]\n",
    "        preds = self.model(ids, mask)\n",
    "        loss = self.criterion(preds.squeeze(1), batch[2])\n",
    "        rmse = self.metric(preds.squeeze(1), batch[2])\n",
    "        logs = {\"val_loss\": loss, \"val_error\": rmse}\n",
    "        self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        ids, mask = batch[0], batch[1]\n",
    "        preds = self.model(ids, mask)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1cb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug == True:\n",
    "    train_data = train_data.iloc[:200]\n",
    "\n",
    "scores = train_data.score.values\n",
    "train_data.drop(\"score\", inplace=True, axis=1)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, scores,\n",
    "                                                                 stratify=scores,\n",
    "                                                                 test_size=CFG.val_size,\n",
    "                                                                 random_state=CFG.seed)\n",
    "\n",
    "train_data[\"score\"] = train_labels\n",
    "val_data[\"score\"] =  val_labels\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "train_dataset = PhraseSimilarityDataset(train_data, tokenizer)\n",
    "\n",
    "val_dataset = PhraseSimilarityDataset(val_data, tokenizer)\n",
    "test_dataset = PhraseSimilarityTestset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5567133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "685591c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c77d624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "538fba23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.steps_per_epoch = len(train_dataloader)\n",
    "CFG.steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "379e63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(save_dir='./', name=CFG.model_name.split('/')[-1]+'_log')\n",
    "logger.log_hyperparams(CFG.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99839970",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor=CFG.monitor,\n",
    "                                     save_top_k=1,\n",
    "                                     save_last=True,\n",
    "                                     save_weights_only=True,\n",
    "                                     filename=\"{epoch:02d}-{valid_loss:.4f}-{valid_acc:.4f}\",\n",
    "                                     verbose=False,\n",
    "                                     mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32083ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=CFG.monitor,\n",
    "                                   patience=CFG.patience,\n",
    "                                   verbose=False,\n",
    "                                   mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fe17950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = PhraseSimilarityModelImpl(CFG.model_name)\n",
    "criterion = nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "metric = MeanSquaredError()\n",
    "driver = PhraseSimilarityModel(model, criterion, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "587e12bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=CFG.epochs,\n",
    "    accumulate_grad_batches=CFG.accumulate,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback], \n",
    "    logger=logger,\n",
    "    weights_summary='top',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60d441f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | model     | PhraseSimilarityModelImpl | 109 M \n",
      "1 | criterion | HuberLoss                 | 0     \n",
      "2 | metric    | MeanSquaredError          | 0     \n",
      "--------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.932   Total estimated model params size (MB)\n",
      "/anaconda/envs/USPTO/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:404: UserWarning: Skipping '__dict__' parameter because it is not possible to safely dump to YAML.\n",
      "  warn(f\"Skipping '{k}' parameter because it is not possible to safely dump to YAML.\")\n",
      "/anaconda/envs/USPTO/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:404: UserWarning: Skipping '__weakref__' parameter because it is not possible to safely dump to YAML.\n",
      "  warn(f\"Skipping '{k}' parameter because it is not possible to safely dump to YAML.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/USPTO/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/anaconda/envs/USPTO/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/anaconda/envs/USPTO/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2bfc21e3ef499da6ba6bd53471d600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(driver, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USPTO",
   "language": "python",
   "name": "uspto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
